.. include:: common.rst

OSCIED-Transform : The Transcoder
---------------------------------

.. seealso::

    You can |browse_transform|_

OSS Tools
^^^^^^^^^

* Celery_ Distributed Task Queue
* FFmpeg_ Complete Multimedia Framework from the FFmpeg Foundation

Introduction
^^^^^^^^^^^^

This component is the worker specialized in handling transformation jobs. In fact this is *celeryd* daemon that handles the requests and maps jobs to transform functions calls. This charm's start hook will launch and connect the daemon to the message broker's queue(s) specified in configuration [#tran1]_.

For example, one can choose that the workers running on his private OpenStack_ cloud will handle *private* & *high priority* transformation requests by setting worker's *rabbit_queues* option to "t_priv,t_high". Then one only need to launch jobs of such kind in one of the defined queues (*t_priv*, *t_high*) and that's it !

Moreover, one can choose to explicitly target a unique worker (e.g. *myWorker1*) by sending jobs to the queue *myWorker1*, this is another interesting feature offered by the application.

.. |components_t| replace:: Architecture of the Transform Unit

.. only:: html

    .. figure:: ../schematics/OSCIED-Components_transform.png
        :width: 1200px
        :align: center
        :alt: |components_t|

        |components_t|

.. only:: latex

    .. figure:: ../schematics/OSCIED-Components_transform.png
        :scale: 80 %
        :alt: |components_t|

        |components_t|

Charm's Configuration
^^^^^^^^^^^^^^^^^^^^^

You can start the charm without specifying any configuration (default values will be used, see :doc:`appendices-transform`) but I strongly recommend to specify your own values in production !

* **verbose** Set verbose logging
* **concurrency** Amount of tasks the worker can handle simultaneously
* **rabbit_queues** Worker connect to queues to receive jobs
* **mongo_connection** Orchestrator database connection [#tran2]_
* **rabbit_connection** Orchestrator message broker connection [#tran2]_
* **storage_ip** Shared storage hostname / IP address (see interface mount of NFS charm) [#tran3]_
* **storage_fstype** Shared storage filesystem type (e.g. NFS) [#tran3]_
* **storage_mountpoint** Shared storage mount point (e.g. for NFS - /srv/data) [#tran3]_
* **storage_options** Shared storage options (e.g. for NFS - rw,sync,no_subtree_check)

.. [#tran1] Add worker's name to queues list, this make possible to launch jobs to this specific worker
.. [#tran2] If all options are set this will override and disable transform relation
.. [#tran3] If all options are set this will override and disable storage relation

Charm's Hooks Activity Diagrams
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. |activity_transform_hooks| replace:: Activity diagram of Transform unit life-cycle hooks

.. only:: html

    .. figure:: ../uml/activity-transform-hooks.png
        :width: 1255px
        :align: center
        :target: juju_unit_startup_
        :alt: |activity_transform_hooks|

        |activity_transform_hooks|

.. only:: latex

    .. figure:: ../uml/activity-transform-hooks.png
        :scale: 100 %
        :target: juju_unit_startup_
        :alt: |activity_transform_hooks|

        |activity_transform_hooks|

Charm's Relations
^^^^^^^^^^^^^^^^^

* Provides : (nothing)
* Requires : Storage [Mount], Transform [Subordinate]

.. warning::

    The unit's daemon will not start until both conditions are fulfilled :

    * A shared storage is mounted (via the storage relation or by specifying it into configuration)
    * An orchestrator is registered (via the transform relation or by specifying it into configuration)

Job State Machine
^^^^^^^^^^^^^^^^^

The orchestrator stores informations about the transformation jobs into database in parallel to the informations that Celery_ also stores in. It may seem as duplicate however I choose to do as such for good reasons :

* Additional informations about the jobs can be stored.
* One can choose to replace Celery_ to use any other task queuing technology.
* Listing of jobs is easy to implement, no needs of Celery_'s inspect & filtering.

Celery_'s keep track of jobs and stores informations about jobs into a *backend*, the orchestrator's database (MongoDB_).

    " During its lifetime a task will transition through several possible states, and each state may have arbitrary metadata attached to it. When a task moves into a new state the previous state is forgotten about, but some transitions can be deducted, (e.g. a task now in the FAILED state, is implied to have been in the STARTED state at some point). " [#tran4]_

So, the transformation jobs stored in database has a *statistic* field that is filled with values mainly generated by the orchestrator such as *add_date*. The state machine diagram shows what is store in this field plus the values that are appended to job's state metadata.

**Remark:** The orchestrator RESTful API transformation methods responses contains job's metadata, appended into *statistic* field.

.. |state_tjob| replace:: State machine of an encoding job (transform -> transform)

.. only:: html

    .. figure:: ../uml/state-tjob.png
        :width: 1014px
        :align: center
        :target: Celery_Tasks_
        :alt: |state_tjob|

        |state_tjob|

.. only:: latex

    .. figure:: ../uml/state-tjob.png
        :scale: 100 %
        :target: Celery_Tasks_
        :alt: |state_tjob|

        |state_tjob|

.. [#tran4] Celery Tasks Page -- http://docs.celeryproject.org/en/latest/userguide/tasks.html

Job Sequence Diagrams
^^^^^^^^^^^^^^^^^^^^^

A Successful Job
++++++++++++++++

.. |sequence_tjobs_workers_success| replace:: Sequence diagram of a successful encoding job (transform -> transform)

.. only:: html

    .. figure:: ../uml/sequence-tjobs-workers_success.png
        :width: 1001px
        :align: center
        :alt: |sequence_tjobs_workers_success|

        |sequence_tjobs_workers_success|

.. only:: latex

    .. figure:: ../uml/sequence-tjobs-workers_success.png
        :scale: 98 %
        :alt: |sequence_tjobs_workers_success|

        |sequence_tjobs_workers_success|

.. raw:: latex

    \newpage
